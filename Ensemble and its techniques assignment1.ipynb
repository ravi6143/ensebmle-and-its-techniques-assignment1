{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7eacef89-856a-4e58-942e-6d33c98e1a39",
   "metadata": {},
   "source": [
    "## Question - 1\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4498e423-38ef-47b8-8928-ba9f048a39fe",
   "metadata": {},
   "source": [
    "Ensemble techniques in machine learning involve combining multiple individual models to create a stronger, more robust, and often more accurate predictive model. The idea is to leverage the diversity of different models and combine their predictions to achieve better overall performance than any single model on its own.\n",
    "\n",
    "* The main types of ensemble techniques are:\n",
    "\n",
    "1. Bagging (Bootstrap Aggregating): It involves training multiple instances of the same learning algorithm on different subsets of the training data. Each model is trained independently, and their predictions are combined, often by averaging (for regression) or voting (for classification). Random Forest is a popular example of a bagging ensemble algorithm.\n",
    "\n",
    "2. Boosting: Boosting focuses on correcting errors made by previous models. Models are trained sequentially, and each subsequent model gives more weight to instances that were misclassified by the previous ones. AdaBoost and Gradient Boosting are common boosting algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec98c70-45ab-4d3a-b99d-9b0c9e16a43f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401881b5-ba48-4fe7-b7e1-c73822952de1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b115e65a-e23b-45a3-b530-cd3b3f7d8aaa",
   "metadata": {},
   "source": [
    "## Question - 2\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c6c9086-6e80-4c5c-8e3c-ed4f87de34bb",
   "metadata": {},
   "source": [
    "1. Improved Accuracy: Ensemble methods often achieve higher predictive accuracy compared to individual models. By combining multiple models, ensemble techniques can compensate for the weaknesses of individual models and leverage their strengths, resulting in better overall performance.\n",
    "\n",
    "2. Robustness: Ensemble methods are more robust to noise and outliers in the data. Since ensemble models aggregate predictions from multiple sources, they tend to be less sensitive to noisy or erroneous data points, leading to more reliable predictions.\n",
    "\n",
    "3. Reduced Overfitting: Ensemble techniques help reduce overfitting, especially when individual models are prone to overfitting. By combining multiple models trained on different subsets of the data or using different algorithms, ensemble methods mitigate the risk of overfitting and improve generalization to unseen data.\n",
    "\n",
    "4. Versatility: Ensemble techniques can be applied to a wide range of machine learning problems and algorithms. They are compatible with various types of models, including decision trees, neural networks, and support vector machines, making them versatile tools in the machine learning toolkit.\n",
    "\n",
    "5. Bias-Variance Tradeoff: Ensemble methods effectively manage the bias-variance tradeoff. By combining models with different biases and variances, ensemble techniques strike a balance between model complexity and generalization performance, resulting in models that are both accurate and robust."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fa6c1f-4d7e-407f-9fef-6b048ef6e47a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5c1c96-b0fc-4dc8-a30c-6414bd59c7e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "93d1df56-f0bf-4c1b-a751-e0ccc5513e81",
   "metadata": {},
   "source": [
    "## Question - 3\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f2bc3e-69a1-4b00-a6fc-0a0374140202",
   "metadata": {},
   "source": [
    "## Bagging : \n",
    "\n",
    "Baggign involves training multilpe instances of the same model using different subsets of the training data. Each subset created by sampling with replacement. The final prediction is often an average in case of regression model and majority of vote of the predicition in case of classification model.\n",
    "\n",
    "* The main steps involved in bagging are as follows:\n",
    "\n",
    "1. Bootstrap Sampling: Bagging starts by creating multiple bootstrap samples of the training data. Bootstrap sampling involves randomly selecting subsets of the training data with replacement. Each bootstrap sample typically has the same size as the original training dataset but may contain duplicate instances.\n",
    "\n",
    "2. Model Training: Once the bootstrap samples are created, a base learning algorithm (e.g., decision tree, neural network) is trained independently on each bootstrap sample. This results in multiple models that have been trained on different subsets of the training data.\n",
    "\n",
    "3. Model Aggregation: After training the individual models, bagging combines their predictions to make a final prediction. The most common aggregation technique is averaging the predictions for regression tasks or taking a majority vote for classification tasks.\n",
    "\n",
    "4. Final Prediction: The final prediction is obtained by aggregating the predictions of all individual models. For regression problems, this could be the mean of the predicted values, while for classification problems, it could be the class with the highest frequency among the predictions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4265900d-d2cc-4aa1-9ae1-b42a0e0560b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a3822f-ee41-4eae-b2d7-dfb96f4381a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd3eea28-d51b-48d9-8cfc-c51d6dc0e8ad",
   "metadata": {},
   "source": [
    "## Question - 4\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f40cb5-8f87-44f1-aa3d-7c61ee94be4b",
   "metadata": {},
   "source": [
    "## Boosting :\n",
    "\n",
    "Boosting is another ensemble technique in machine learning that aims to improve the performance of predictive models by combining the predictions of multiple weak learners (typically simple models) to create a strong learner. Unlike bagging, which trains multiple models independently and combines their predictions, boosting trains a sequence of models sequentially, with each subsequent model focusing on the instances that were misclassified by the previous models. \n",
    "\n",
    "* The main steps involved in boosting are as follows:\n",
    "\n",
    "1. Sequential Training: Boosting starts by training a base learning algorithm (e.g., decision tree, shallow neural network) on the entire training dataset. This initial model is referred to as the weak learner.\n",
    "\n",
    "2. Weighted Instances: After training the initial weak learner, boosting assigns weights to the training instances based on their performance. Instances that were misclassified by the weak learner are assigned higher weights, while correctly classified instances are assigned lower weights.\n",
    "\n",
    "3. Sequential Model Training: Boosting then iteratively trains additional weak learners, each focusing on the instances that were misclassified by the previous models. During each iteration, the algorithm adjusts the weights of the training instances to emphasize the misclassified instances, effectively \"boosting\" their importance in subsequent training iterations.\n",
    "\n",
    "4. Model Combination: Once all weak learners have been trained, boosting combines their predictions using a weighted sum or a weighted voting scheme. The weights assigned to each weak learner's prediction are typically based on its performance during training.\n",
    "\n",
    "5. Final Prediction: The final prediction is obtained by aggregating the predictions of all weak learners. For regression problems, this could be the weighted average of the predicted values, while for classification problems, it could be the class with the highest weighted vote among the predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66cb4b34-fa65-4804-9edf-25fb8b49b15f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81db2224-2c99-4186-a69a-9ea3b851eaca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac6a6ba-a0c9-4f67-8984-4d60a45a650c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36ac65f3-3fbc-457b-9b79-45febff0ba7b",
   "metadata": {},
   "source": [
    "## Question - 5\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61975082-0866-4097-8c01-2199d083447d",
   "metadata": {},
   "source": [
    "1. Improved Predictive Performance: Ensemble methods typically yield higher predictive accuracy compared to individual base models. By combining multiple models, ensemble techniques can effectively mitigate errors and enhance the overall performance of the predictive model.\n",
    "\n",
    "2. Robustness and Stability: Ensemble methods are less prone to overfitting, especially when using techniques like bagging and boosting. By averaging or combining the predictions of multiple models, ensembles can generalize better to unseen data and handle noise and outliers more effectively.\n",
    "\n",
    "3. Bias-Variance Tradeoff: Ensemble techniques help balance the bias-variance tradeoff by leveraging the strengths of different models. For example, bagging reduces variance by averaging the predictions of multiple models, while boosting reduces bias by focusing on misclassified instances.\n",
    "\n",
    "4. Increased Diversity: Ensemble methods rely on combining diverse models, which can be achieved through different algorithms, feature subsets, or training data subsets. This diversity helps capture different aspects of the underlying data distribution and leads to more robust predictions.\n",
    "\n",
    "5. Versatility: Ensemble techniques are versatile and can be applied to various types of machine learning tasks, including classification, regression, and anomaly detection. They can be used with different types of base models and are not limited to any specific algorithm.\n",
    "\n",
    "6. Scalability: Many ensemble methods, such as bagging and random forests, are inherently parallelizable, making them suitable for large-scale datasets and distributed computing environments. This scalability allows ensemble techniques to handle big data efficiently.\n",
    "\n",
    "7. Interpretability: While individual base models in an ensemble may not always be interpretable, ensemble predictions can often be more interpretable than those of individual models. Techniques like feature importance in random forests or gradient boosting can provide insights into the relative importance of different features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9d4368-7096-4bb5-8784-814d0c08ab4a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90cdf598-07d1-4d28-bfd5-a10ae6e3e5a2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f31b33b-28cd-47ae-b288-f178321d7b90",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "948217a8-20c0-4f8b-bbda-d34c9a2e09d6",
   "metadata": {},
   "source": [
    "## Question - 6\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b81776-414f-4c78-ac99-662435d376df",
   "metadata": {},
   "source": [
    "Ensemble techniques are powerful tools in machine learning, but whether they are always better than individual models depends on various factors:\n",
    "\n",
    "1. Data Quality: If the dataset is small or noisy, individual models may perform better than ensemble methods. Ensemble techniques rely on combining diverse models, so if the base models are trained on poor-quality data or suffer from high bias, ensembles may not yield significant improvements.\n",
    "\n",
    "2. Model Diversity: The effectiveness of ensemble methods hinges on the diversity of the base models. If the base models are highly correlated or similar in nature, the ensemble may not offer significant performance gains. Ensuring diversity among the base models is crucial for the success of ensemble techniques.\n",
    "\n",
    "3. Computational Resources: Ensemble methods are typically more computationally intensive than individual models, especially when training large ensembles or using complex algorithms like gradient boosting. In situations where computational resources are limited, using simpler models or individual models may be more practical.\n",
    "\n",
    "4. Interpretability: Individual models are often more interpretable than ensemble methods, especially when considering models like decision trees or linear regression. If interpretability is a priority, using individual models may be preferable over ensembles, which tend to be more complex.\n",
    "\n",
    "5. Problem Complexity: For complex problems with intricate relationships between features, ensemble techniques are often more effective at capturing the underlying patterns than individual models. However, for simpler problems with well-defined patterns, individual models may suffice and could be easier to interpret and implement.\n",
    "\n",
    "6. Training Time: Ensemble methods typically require more time to train than individual models, especially if the ensemble consists of a large number of base models or if the base models are computationally expensive. In time-sensitive applications, the overhead of training an ensemble may outweigh its benefits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77190e0c-cc65-4a13-a849-f8f12a5cf33e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e82e3b-1422-4448-b041-6e20c08dbf7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c2ef53-9f9c-4e7b-8ebb-5a7a80f844cf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8fdf064a-e2ad-4f00-bbbb-ac015ba72940",
   "metadata": {},
   "source": [
    "## Question - 7\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac71ac7-5d2b-4e84-8839-91d8533b6802",
   "metadata": {},
   "source": [
    "1. Sampling with Replacement:\n",
    "\n",
    "In bootstrap resampling, we randomly select observations from the original sample with replacement. Each bootstrap sample has the same size as the original sample.\n",
    "\n",
    "\n",
    "2. Statistic Calculation:\n",
    "\n",
    "For each bootstrap sample, we calculate the statistic of interest (e.g., sample mean, median, standard deviation). This step simulates the sampling variability and provides an estimate of the sampling distribution of the statistic.\n",
    "\n",
    "\n",
    "3. Bootstrap Distribution:\n",
    "\n",
    "By repeating the resampling process multiple times (often thousands of times), we obtain a distribution of the computed statistics. This distribution represents the possible values of the statistic under different samples drawn from the population.\n",
    "4. Confidence Interval Estimation:\n",
    "\n",
    "To construct a confidence interval, we determine the percentiles of the bootstrap distribution. The most common approach is to find the α/2 and \n",
    "1−α/2 percentiles, where α is the significance level (e.g., 0.05 for a 95% confidence interval).\n",
    "\n",
    "The confidence interval is then formed by the values corresponding to these percentiles.\n",
    "\n",
    "\n",
    "* Example:\n",
    "Suppose we want to estimate the 95% confidence interval for the mean height of a population using bootstrap.\n",
    "\n",
    "We generate bootstrap samples, calculate the mean height for each sample, and obtain a distribution of sample means.\n",
    "The 2.5th and 97.5th percentiles of this distribution represent the lower and upper bounds of the 95% confidence interval, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf8ec4f-a352-4a17-9c3b-eccabc990d78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd75f6e-940e-4eec-bc93-1613070725f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6e45226a-87c3-4494-a9a2-7978b2ccf6fa",
   "metadata": {},
   "source": [
    "\n",
    "## Question - 8\n",
    "ans - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decff08c-f936-416c-898e-ebbba0fd1b91",
   "metadata": {},
   "source": [
    "1. Sample with Replacement:\n",
    "\n",
    "* Start with the observed dataset, which typically consists of n data points.\n",
    "\n",
    "* Generate multiple bootstrap samples by randomly selecting n data points from the observed dataset with replacement. Each bootstrap sample has the same size as the original dataset.\n",
    "\n",
    "2. Compute Statistic:\n",
    "\n",
    "* For each bootstrap sample, compute the statistic of interest (e.g., mean, median, standard deviation). This statistic can be any measure of interest that characterizes the dataset (population parameter or sample statistic).\n",
    "\n",
    "\n",
    "3. Repeat Sampling:\n",
    "\n",
    "* Repeat the sampling process (Step 1) a large number of times (often thousands of times) to obtain a sufficient number of bootstrap samples.\n",
    "\n",
    "4. Calculate Bootstrap Distribution:\n",
    "\n",
    "* Create a distribution of the computed statistics from the bootstrap samples. This distribution represents the variability of the statistic under different samples drawn from the observed data.\n",
    "\n",
    "5. Analyze Distribution:\n",
    "\n",
    "Analyze the bootstrap distribution to make statistical inferences or estimates. This may involve calculating summary statistics (e.g., mean, median, standard deviation) or constructing confidence intervals.\n",
    "\n",
    "6. Interpret Results:\n",
    "\n",
    "Interpret the results obtained from the bootstrap analysis. This could include estimating parameters of interest, making comparisons between groups, or assessing the uncertainty associated with a parameter estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a66bf4-a79e-409d-b9c0-cc65723184e6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a842caaa-14b1-4979-b437-b35fb5db9463",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d08956e-5a3f-4e5b-96a3-ca5f7272be43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec7e191e-2a56-4b23-87fe-93273288f7e6",
   "metadata": {},
   "source": [
    "## Question - 9\n",
    "ans - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c49bc748-0241-490a-a8ed-19ab337be21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95% Confidence Interval for Population Mean Height (meters): [14.45022824 15.54289691]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "sample_mean_height = 15\n",
    "sample_std = 2\n",
    "sample_size = 50\n",
    "\n",
    "number_of_bootstrap_sample = 10000\n",
    "\n",
    "bootstrap_means = []\n",
    "\n",
    "for i in range(number_of_bootstrap_sample):\n",
    "    bootstrap_sample = np.random.normal(loc = sample_mean_height , scale= sample_std , size= sample_size)\n",
    "    \n",
    "    bootstrap_mean = np.mean(bootstrap_sample)\n",
    "    bootstrap_means.append(bootstrap_mean)\n",
    "    \n",
    "    \n",
    "confidence_interval = np.percentile(bootstrap_means , [2.5 , 97.5])\n",
    "\n",
    "print(\"95% Confidence Interval for Population Mean Height (meters):\", confidence_interval)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16821430-d727-4ac0-8294-74fa84c81888",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
